{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using skimage's marching cubes implementation.\n",
      "load grid...\n",
      "compute code from grid...\n"
     ]
    }
   ],
   "source": [
    "from numpy.lib.function_base import kaiser\n",
    "import torch\n",
    "import pickle\n",
    "# import json\n",
    "import yaml\n",
    "import glob\n",
    "import imageio\n",
    "from skimage import color\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from marching_cubes_rgb import *\n",
    "from utils import *\n",
    "import IPython\n",
    "\n",
    "DEFAULT_RENDER_RESOLUTION = 24\n",
    "DEFAULT_MAX_MODEL_2_RENDER = 4\n",
    "# DEFAULT_MAX_MODEL_2_RENDER = None\n",
    "DEFAULT_IMAGES_PER_MODEL = 1\n",
    "\n",
    "DECODER_PATH = \"models_and_codes/decoder.pth\"\n",
    "ENCODER_PATH = \"models_and_codes/encoderGrid.pth\"\n",
    "LATENT_CODE_PATH = \"models_and_codes/latent_code.pkl\"\n",
    "PARAM_FILE = \"config/param_encoder.yaml\"\n",
    "VEHICLE_VALIDATION_PATH = \"config/vehicle_validation.txt\"\n",
    "MATRIX_PATH = \"../../image2sdf/input_images_validation/matrix_w2c.pkl\"\n",
    "ANNOTATIONS_PATH = \"../../image2sdf/input_images_validation/annotations.pkl\"\n",
    "IMAGES_PATH = \"../../image2sdf/input_images_validation/images/\"\n",
    "OUTPUT_DIR = \"../../image2sdf/encoder_output/evaluation\"\n",
    "LOGS_PATH = \"../../image2sdf/logs/encoder/log.pkl\"\n",
    "PLOT_PATH = \"../../image2sdf/plots/encoder/\"\n",
    "\n",
    "def init_xyz(resolution):\n",
    "    xyz = torch.empty(resolution * resolution * resolution, 3).cuda()\n",
    "\n",
    "    for x in range(resolution):\n",
    "        for y in range(resolution):\n",
    "            for z in range(resolution):\n",
    "                xyz[x * resolution * resolution + y * resolution + z, :] = torch.Tensor([x/(resolution-1)-0.5,y/(resolution-1)-0.5,z/(resolution-1)-0.5])\n",
    "\n",
    "    return xyz\n",
    "\n",
    "\n",
    "def load_grid(list_hash, annotations, num_model_2_render, param_image, param_network):\n",
    "\n",
    "    matrix_world_to_camera = pickle.load(open(MATRIX_PATH, 'rb'))\n",
    "\n",
    "    num_model = len(list_hash)\n",
    "\n",
    "    width_image = param_image[\"width\"]\n",
    "    height_image = param_image[\"height\"]\n",
    "    width_network = param_network[\"width\"]\n",
    "    height_network = param_network[\"height\"]\n",
    "    num_slices = param_network[\"num_slices\"]\n",
    "\n",
    "    \n",
    "    all_grid = torch.empty([num_model, num_model_2_render, 3, num_slices, width_network, height_network], dtype=torch.float)\n",
    "\n",
    "    list_id = list(annotations.keys())\n",
    "\n",
    "    for model_hash, model_id in zip(list_hash, range(num_model)):\n",
    "        for image_pth, image_id in zip(glob.glob(IMAGES_PATH + model_hash + '/*'), range(num_model_2_render)):\n",
    "\n",
    "            # Load data and get label\n",
    "            image_pth = IMAGES_PATH + model_hash + '/' + str(image_id) + '.png'\n",
    "            input_im = imageio.imread(image_pth)\n",
    "\n",
    "            loc_3d = annotations[model_hash][image_id]['3d'].copy()\n",
    "            frame = annotations[model_hash][image_id]['frame'].copy()\n",
    "\n",
    "            # interpolate slices vertex coordinates\n",
    "            loc_slice_3d = np.empty([num_slices,4,3])\n",
    "            for i in range(num_slices):\n",
    "                loc_slice_3d[i,0,:] = loc_3d[0,:] * (1-i/(num_slices-1)) + loc_3d[4,:] * i/(num_slices-1)\n",
    "                loc_slice_3d[i,1,:] = loc_3d[1,:] * (1-i/(num_slices-1)) + loc_3d[5,:] * i/(num_slices-1)\n",
    "                loc_slice_3d[i,2,:] = loc_3d[2,:] * (1-i/(num_slices-1)) + loc_3d[6,:] * i/(num_slices-1)\n",
    "                loc_slice_3d[i,3,:] = loc_3d[3,:] * (1-i/(num_slices-1)) + loc_3d[7,:] * i/(num_slices-1)\n",
    "\n",
    "            # convert to image plane coordinate\n",
    "            loc_slice_2d = np.empty_like(loc_slice_3d)\n",
    "            for i in range(num_slices):\n",
    "                for j in range(4):\n",
    "                    loc_slice_2d[i,j,:] = convert_w2c(matrix_world_to_camera, frame, loc_slice_3d[i,j,:]) \n",
    "\n",
    "            ###### y coordinate is inverted + rescaling #####\n",
    "            loc_slice_2d[:,:,1] = 1 - loc_slice_2d[:,:,1]\n",
    "            loc_slice_2d[:,:,0] = loc_slice_2d[:,:,0] * width_image\n",
    "            loc_slice_2d[:,:,1] = loc_slice_2d[:,:,1] * height_image\n",
    "\n",
    "            # grid to give as input to the network\n",
    "            input_grid = np.empty([num_slices, width_network, height_network, 3])\n",
    "\n",
    "\n",
    "            # fill grid by slices\n",
    "            for i in range(num_slices):\n",
    "                src = loc_slice_2d[i,:,:2].copy()\n",
    "                dst = np.array([[0, height_network], [width_network, height_network], [width_network, 0], [0,0]])\n",
    "                h, mask = cv2.findHomography(src, dst)\n",
    "                slice = cv2.warpPerspective(input_im, h, (width_network,height_network))\n",
    "                input_grid[i,:,:,:] = slice\n",
    "\n",
    "            # rearange, normalize and convert to tensor\n",
    "            input_grid = np.transpose(input_grid, [3,0,1,2])\n",
    "            input_grid = input_grid/255 - 0.5\n",
    "            input_grid = torch.tensor(input_grid, dtype = torch.float)\n",
    "\n",
    "            all_grid[model_id, image_id, :, :, :, :] = input_grid\n",
    "\n",
    "    return all_grid\n",
    "\n",
    "\n",
    "def get_code_from_grid(grid, latent_size):\n",
    "\n",
    "    encoder = torch.load(ENCODER_PATH).cuda()\n",
    "    encoder.eval()\n",
    "\n",
    "    num_model = grid.shape[0]\n",
    "    num_images_per_model = grid.shape[1]\n",
    "\n",
    "    lat_code = torch.empty([num_model, num_images_per_model, latent_size]).cuda()\n",
    "    for model_id in range(num_model):\n",
    "        for image_id in range(num_images_per_model):\n",
    "            lat_code[model_id, image_id, :] = encoder(grid[model_id, image_id, :, :, :, :].unsqueeze(0).cuda()).detach()\n",
    "\n",
    "\n",
    "    return lat_code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load parameters\n",
    "param_all = yaml.safe_load(open(PARAM_FILE))\n",
    "param = param_all[\"encoder\"]\n",
    "\n",
    "# load annotations\n",
    "annotations = pickle.load(open(ANNOTATIONS_PATH, \"rb\"))\n",
    "dict_hash_2_code = pickle.load(open(LATENT_CODE_PATH, 'rb'))\n",
    "\n",
    "# Get validation model\n",
    "with open(VEHICLE_VALIDATION_PATH) as f:\n",
    "    list_hash_validation = f.read().splitlines()\n",
    "list_hash_validation = list(list_hash_validation)\n",
    "\n",
    "if DEFAULT_MAX_MODEL_2_RENDER is not None:\n",
    "    list_hash_validation = list_hash_validation[:DEFAULT_MAX_MODEL_2_RENDER]\n",
    "\n",
    "# only keep the ones for which with have annotated images\n",
    "list_hash = []\n",
    "for hash in list_hash_validation:\n",
    "    if hash in annotations.keys():\n",
    "        list_hash.append(hash)\n",
    "\n",
    "num_model = len(list_hash)\n",
    "num_images_per_model = len(annotations[list_hash[0]])\n",
    "num_model_2_render= min(num_images_per_model, DEFAULT_IMAGES_PER_MODEL)\n",
    "\n",
    "resolution = DEFAULT_RENDER_RESOLUTION\n",
    "\n",
    "# compute latent codes\n",
    "print(\"load grid...\")\n",
    "grid = load_grid(list_hash, annotations, num_model_2_render, param[\"image\"], param[\"network\"])\n",
    "print(\"compute code from grid...\")\n",
    "latent_code = get_code_from_grid(grid, param_all[\"latent_size\"])\n",
    "\n",
    "\n",
    "# fill a xyz grid to give as input to the decoder \n",
    "xyz = init_xyz(resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loic/MasterPDM/codePDM/img2sdf_code/marching_cubes_rgb.py:100: FutureWarning: marching_cubes_lewiner is deprecated in favor of marching_cubes. marching_cubes_lewiner will be removed in version 0.19\n",
      "  vertices, faces, normals, values = measure.marching_cubes_lewiner(tensor.transpose(1, 0, 2), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 0:\n",
      "cham sdf init: 0.15586727857589722\n",
      "cham sdf final: 0.02146562933921814\n",
      "\n",
      "model 1:\n",
      "cham sdf init: 0.40602123737335205\n",
      "cham sdf final: 0.05332350730895996\n",
      "\n",
      "model 2:\n",
      "cham sdf init: 0.42142486572265625\n",
      "cham sdf final: 0.05141950398683548\n",
      "\n",
      "model 3:\n",
      "cham sdf init: 0.06774818152189255\n",
      "cham sdf final: 0.011309737339615822\n"
     ]
    }
   ],
   "source": [
    "# load decoder\n",
    "decoder = torch.load(DECODER_PATH).cuda()\n",
    "decoder.eval()\n",
    "\n",
    "model_id = 0\n",
    "model_hash = list_hash[model_id]\n",
    "\n",
    "\n",
    "for model_hash, model_id in zip(list_hash, range(num_model)):\n",
    "\n",
    "    code_gt = dict_hash_2_code[model_hash].cuda()\n",
    "\n",
    "    code_prediction = latent_code[model_id,:,:].mean(dim=0)\n",
    "    # code_prediction = torch.zeros(6).cuda()\n",
    "\n",
    "    sdf_pred = decoder(code_prediction.repeat(resolution * resolution * resolution, 1).cuda(),xyz)\n",
    "    sdf_gt = decoder(code_gt.repeat(resolution * resolution * resolution, 1).cuda(),xyz)\n",
    "\n",
    "\n",
    "    sdf_pred[:,1:] = sdf_pred[:,1:] * 255\n",
    "    sdf_pred = sdf_pred.reshape(resolution, resolution, resolution, 4).cpu().detach().numpy()\n",
    "    if(np.min(sdf_pred[:,:,:,0]) < 0 and np.max(sdf_pred[:,:,:,0]) > 0):\n",
    "        vertices_pred, faces_pred = marching_cubes(sdf_pred[:,:,:,0])\n",
    "        colors_v_pred = exctract_colors_v(vertices_pred, sdf_pred)\n",
    "\n",
    "    vertices_pred = torch.tensor(vertices_pred.copy())\n",
    "    colors_v_pred = torch.tensor(colors_v_pred/255).unsqueeze(0).cuda()\n",
    "\n",
    "\n",
    "    sdf_gt[:,1:] = sdf_gt[:,1:] * 255\n",
    "    sdf_gt = sdf_gt.reshape(resolution, resolution, resolution, 4).cpu().detach().numpy()\n",
    "    if(np.min(sdf_gt[:,:,:,0]) < 0 and np.max(sdf_gt[:,:,:,0]) > 0):\n",
    "        vertices_gt, faces_gt = marching_cubes(sdf_gt[:,:,:,0])\n",
    "        colors_v_gt = exctract_colors_v(vertices_gt, sdf_gt)\n",
    "\n",
    "    vertices_gt = torch.tensor(vertices_gt.copy())\n",
    "    colors_v_gt = torch.tensor(colors_v_gt/255).unsqueeze(0).cuda()\n",
    "\n",
    "    cham_sdf, cham_rgb, cham_lab = chamfer_distance_rgb(vertices_pred, vertices_gt, colors_x = colors_v_pred, colors_y = colors_v_gt)\n",
    "\n",
    "\n",
    "    vertices_pred.requires_grad = True\n",
    "    vertices_gt.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": vertices_pred,\n",
    "            \"lr\": 0.1,\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"params\": colors_v_pred,\n",
    "            \"lr\": 0.1,\n",
    "        },\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nmodel {model_id}:\")\n",
    "    print(f\"cham sdf init: {cham_sdf.item()}\")\n",
    "\n",
    "    for i in range(10):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cham_sdf, cham_rgb, cham_lab = chamfer_distance_rgb(vertices_pred, vertices_gt, colors_x = colors_v_pred, colors_y = colors_v_gt)\n",
    "\n",
    "        # print(cham_rgb.item())\n",
    "\n",
    "        cham_sdf.backward()\n",
    "        # cham_rgb.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"cham sdf final: {cham_sdf.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n",
      "54.75667190551758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    cham_sdf, cham_rgb, cham_lab = chamfer_distance_rgb(vertices_pred, vertices_gt, colors_x = colors_v_pred, colors_y = colors_v_gt)\n",
    "\n",
    "    # print(cham_sdf.item())\n",
    "    print(cham_rgb.item())\n",
    "\n",
    "    # cham_sdf.backward()\n",
    "    cham_rgb.requires_grad = True\n",
    "    cham_rgb.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model 0:\n",
      "total loss: 0.06550651788711548\n",
      "distance to the original code 0.871015727519989 \n",
      "\n",
      "model 1:\n",
      "total loss: 0.04519502446055412\n",
      "distance to the original code 0.7388850450515747 \n",
      "\n",
      "model 2:\n",
      "total loss: 0.06612680852413177\n",
      "distance to the original code 0.7353546619415283 \n",
      "\n",
      "model 3:\n",
      "total loss: 0.01805618777871132\n",
      "distance to the original code 0.2888331115245819 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load decoder\n",
    "decoder = torch.load(DECODER_PATH).cuda()\n",
    "decoder.eval()\n",
    "\n",
    "# model_id = 0\n",
    "# model_hash = list_hash[model_id]\n",
    "\n",
    "\n",
    "for model_hash, model_id in zip(list_hash, range(num_model)):\n",
    "\n",
    "    code_gt = dict_hash_2_code[model_hash].cuda()\n",
    "\n",
    "    code_prediction = latent_code[model_id,:,:].mean(dim=0)\n",
    "    # code_prediction = torch.zeros(6).cuda()\n",
    "\n",
    "    code_prediction.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\n",
    "            \"params\": code_prediction,\n",
    "            \"lr\": 0.05,\n",
    "            \"eps\": 1e-8,\n",
    "        },\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    for i in range(20):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        sdf_pred = decoder(code_prediction.repeat(resolution * resolution * resolution, 1).cuda(),xyz)\n",
    "        sdf_gt = decoder(code_gt.repeat(resolution * resolution * resolution, 1).cuda(),xyz)\n",
    "\n",
    "        # assign weight of 0 for easy samples that are well trained\n",
    "        threshold_precision = 1\n",
    "        weight_sdf = ~((sdf_pred[:,0] > threshold_precision).squeeze() * (sdf_gt[:,0] > threshold_precision).squeeze()) \\\n",
    "            * ~((sdf_pred[:,0] < -threshold_precision).squeeze() * (sdf_gt[:,0] < -threshold_precision).squeeze())\n",
    "\n",
    "        # loss l1 in distance error per samples\n",
    "        loss_sdf = torch.nn.L1Loss(reduction='none')(sdf_pred[:,0].squeeze(), sdf_gt[:,0])\n",
    "        loss_sdf = (loss_sdf * weight_sdf).mean() * weight_sdf.numel()/weight_sdf.count_nonzero()\n",
    "\n",
    "        # loss rgb in pixel value difference per color per samples\n",
    "        rgb_gt_normalized = sdf_gt[:,1:]\n",
    "        loss_rgb = torch.nn.L1Loss(reduction='none')(sdf_pred[:,1:], rgb_gt_normalized)\n",
    "        loss_rgb = ((loss_rgb[:,0] * weight_sdf) + (loss_rgb[:,1] * weight_sdf) + (loss_rgb[:,2] * weight_sdf)).mean()/3 * weight_sdf.numel()/weight_sdf.count_nonzero()\n",
    "\n",
    "\n",
    "        total_loss = loss_sdf + loss_rgb\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(f\"\\nmodel {model_id}:\")\n",
    "    print(f\"total loss: {total_loss}\")\n",
    "    print(f\"distance to the original code {(code_prediction - code_gt).norm().item()} \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
